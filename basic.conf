#master node
[cluster]
CLUSTER_USER = hadoop
CLUSTER_NAME = a0
CLUSTER_NODE_LIST = a1,a2,a3,a4,a5

#the master and worker must keep same directory on "app" and "data"
CLUSTER_APP_DIR = /bio/app
CLUSTER_DATA_DIR = /bio/data
CLUSTER_PIPELINE_DIR=/bio/pipeline
#CLUSTER_MASTER_PIPELINE_DIR = /bio/pipeline

#master
CLUSTER_MASTER_JOB_DIR = /home/hadoop/pbs/job
CLUSTER_MASTER_LOG_DIR = /home/hadoop/pbs/log
CLUSTER_MASTER_TMP_DIR = /home/hadoop/pbs/job


#workers
CLUSTER_NODE_JOB_DIR = /home/hadoop/pbs
CLUSTER_NODE_TMP_DIR = /home/hadoop/pbs

#can use either the tmp dir on master(larger disk capacity) or node (better performance)

[localserver]
LOCAL_SERVER_USER = bioservice
LOCAL_SERVER_NAME = tango
LOCAL_SERVER_JOB_DIR = /project

LOCAL_SERVER_APP_DIR = /bio/app
LOCAL_SERVER_DATA_DIR = /bio/data
LOCAL_SERVER_PIPELINE_DIR = /bio/pipeline


[pbs]
#seconds. Query if a job was completed in this interval.
PBS_QUERY_INTERVAL_SECONDS = 10

#hours
MAX_WALLTIME_HOURS = 240 

[misc]
SMTP_SERVER=mail.fluidigm.com
SMTP_SERVER_USER=fluidigm\bioinformaticsService
SMTP_SERVER_PASSWORD=B10inform@tics

#RE_PAIRED_END_READ_FILE_1 = (.*)_1\.(.+)
#RE_PAIRED_END_READ_FILE_2 = (.*)_2\.(.+)
RE_PAIRED_END_READ_FILE_1 = (.*)[\._][Rr]?1(.*?).f(ast)?q(.gz)?
RE_PAIRED_END_READ_FILE_2 = (.*)[\._][Rr]?2(.*?).f(ast)?q(.gz)?

[rsync]
#seconds
RSYNC_QUERY_INTERVAL_SECONDS  = 10 

[file]
#command file
FILE_CMD           = cmd

#standard output file
FILE_STDOUT        = stdout 

#standard error file
FILE_STDERR        = stderr
 
#log file
FILE_LOG           = log
 
#empty file as "abort a job" place holder
FILE_TAG_ABORT     = a 

#empty file as "begin a job" place holder
FILE_TAG_BEGIN     = b 

#empty file as "running a job" place holder
FILE_TAG_RUNNING   = r 

#empty file as "dry-run a job (only list applications to be executed)" place holder
FILE_TAG_DRYRUN   = d 

