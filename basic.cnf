#master node
[cluster]
CLUSTER_USER = hadoop
CLUSTER_NAME = a0

CLUSTER_MASTER_DIR = /bio
CLUSTER_MASTER_TMP_DIR = /bio/tmp

#computation node
CLUSTER_NODE_LIST = a1,a2,a3,a4,a5
CLUSTER_NODE_DIR = /bio/pbs
CLUSTER_NODE_TMP_DIR = /bio/pbs/tmp

#can use either the tmp dir on master(larger disk capacity) or node (better performance)
CLUSTER_TMP_DIR = CLUSTER_NODE_TMP_DIR 

[localserver]
LOCAL_SERVER_USER = hadoop
LOCAL_SERVER_NAME = scheduler
LOCAL_SERVER_DIR = /bio


[pbs]
#seconds. Query if a job was completed in this interval.
PBS_QUERY_INTERVAL_SECONDS = 10

#hours
MAX_WALLTIME_HOURS = 240 

[misc]
SMTP_SERVER=smtp.fluidigm.com
SMTP_SERVER_USER=bioinformaticsservice
SMTP_SERVER_PASSWORD=13572468

#RE_PAIRED_END_READ_FILE_1 = (.*)_1\.(.+)
#RE_PAIRED_END_READ_FILE_2 = (.*)_2\.(.+)
RE_PAIRED_END_READ_FILE_1 = (.*)[\._][Rr]?1(.*?).f(ast)?q(.gz)?
RE_PAIRED_END_READ_FILE_2 = (.*)[\._][Rr]?2(.*?).f(ast)?q(.gz)?

#generally you should not change any following variables unless you completely understand how it works
[path]
PATH_APP = app
PATH_DATA = data
PATH_LOG = log
PATH_JOB = job
PATH_CONF = conf

[rsync]
#seconds, query remote data server for new data
RSYNC_QUERY_INTERVAL_SECONDS  = 10 

[file]
#commands of each job
FILE_CMD           = cmd.txt

#standard output and error of each job
FILE_STDOUT        = stdout.txt 

#standard output and error of each job
FILE_STDERR        = stderr.txt
 
#log information of each job
FILE_LOG           = log.txt
 
#empty file as "abort a job" place holder
FILE_TAG_ABORT     = a 

#empty file as "begin processing a job in parallel" place holder
FILE_TAG_PARALLEL     = p

#empty file as "begin a job" place holder
FILE_TAG_BEGIN     = b 

#empty file as "running a job" place holder
FILE_TAG_RUNNING   = r 

#empty file as "dry-run a job (only list applications to be executed)" place holder
FILE_TAG_DRYRUN   = d 

